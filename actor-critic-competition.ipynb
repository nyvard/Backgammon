{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import Backgammon as B\n",
    "import agent as A\n",
    "import flipped_agent as FA\n",
    "\n",
    "import pubeval_tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class backgammon:\n",
    "    def __init__(self):\n",
    "        self.board = B.init_board()\n",
    "            \n",
    "    def reset(self):\n",
    "        self.board = B.init_board()\n",
    "        self.done = False\n",
    "        \n",
    "    def choose_board(self, board):\n",
    "        self.board = board\n",
    "        self.done = False\n",
    "        return np.copy(self.board)\n",
    "    \n",
    "    def legal_moves(self, dice, player):\n",
    "        moves, boards = B.legal_moves(board = self.board, dice = dice, player = player)\n",
    "        if len(boards) == 0:\n",
    "            return [], self.board\n",
    "        n_boards = np.shape(boards)[0]\n",
    "        tesauro = np.zeros((n_boards, 198))\n",
    "        for b in range(n_boards):\n",
    "            tesauro[b,:] = features(boards[b], player)\n",
    "        return moves, tesauro\n",
    "    \n",
    "    def swap_player(self):\n",
    "        self.board = FA.flip_board(board_copy=np.copy(self.board))\n",
    "    \n",
    "    # oppents random move\n",
    "    def make_move(self, dice):\n",
    "        moves, _ = self.legal_moves(dice, -1)\n",
    "        if len(moves) == 0:\n",
    "            return self.step([], -1)\n",
    "        move = moves[np.random.randint(len(moves))]\n",
    "        return self.step(move, -1)\n",
    "    \n",
    "    def step(self, move, player):\n",
    "        if len(move) != 0:\n",
    "            for m in move:\n",
    "                self.board = B.update_board(board = self.board, move = m, player = player)\n",
    "        reward = 0\n",
    "        self.done = False\n",
    "        if self.iswin():\n",
    "            reward = player\n",
    "            self.done = True\n",
    "        tesauro_board = features(np.copy(self.board), player)\n",
    "        return tesauro_board, reward, self.done\n",
    "        \n",
    "    def iswin(self):\n",
    "        return B.game_over(self.board)\n",
    "        \n",
    "    def render(self):\n",
    "        B.pretty_print(self.board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph(seed=43):\n",
    "    #tf.reset_default_graph()\n",
    "    #tf.set_random_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "D_in, H1, H2, D_out = 198, 256, 128, 1\n",
    "\n",
    "actor = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H1),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(H1, H2),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(H2, D_out),\n",
    "    torch.nn.Softmax(dim=0),\n",
    ")\n",
    "critic = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H1),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(H1, H2),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(H2, D_out),\n",
    "    torch.nn.Tanh(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_and_value(actor, boards):\n",
    "    boards = torch.from_numpy(boards).float()\n",
    "    possible_actions_probs = actor(boards)\n",
    "    with torch.no_grad():\n",
    "        action = int(torch.multinomial(possible_actions_probs.view(1,-1), 1))\n",
    "    action_value = possible_actions_probs[action]\n",
    "    return action, action_value\n",
    "\n",
    "def get_action_and_value_greedy(actor, boards):\n",
    "    boards = torch.from_numpy(boards).float()\n",
    "    possible_actions_probs = actor(boards)\n",
    "    action = np.argmax(possible_actions_probs)\n",
    "    action_value = possible_actions_probs[action]\n",
    "    return action, action_value\n",
    "\n",
    "def get_action_value(actor, boards, action):\n",
    "    boards = torch.from_numpy(boards).float()\n",
    "    possible_actions_probs = actor(boards)\n",
    "    action_value = possible_actions_probs[action]\n",
    "    return action_value\n",
    "\n",
    "def get_action(actor, boards):\n",
    "    with torch.no_grad():\n",
    "        boards = torch.from_numpy(boards).float()\n",
    "        possible_actions_probs = actor(boards)\n",
    "        action = torch.multinomial(possible_actions_probs.view(1,-1), 1)\n",
    "    return int(action)\n",
    "\n",
    "def get_action_greedy(actor, boards):\n",
    "    with torch.no_grad():\n",
    "        boards = torch.from_numpy(boards).float()\n",
    "        possible_actions_probs = actor(boards)\n",
    "        action = np.argmax(possible_actions_probs)\n",
    "    return int(action)\n",
    "\n",
    "def get_state_value(nn_model, after_state):\n",
    "    after_state = torch.from_numpy(after_state).float()\n",
    "    value = nn_model(after_state)\n",
    "    return value\n",
    "\n",
    "def epsilon_greedy(critic, possible_boards, epsilon=1):\n",
    "    possible_boards = torch.from_numpy(possible_boards).float()\n",
    "    values = critic(possible_boards)\n",
    "    if np.random.random()<epsilon:\n",
    "        _ , index = values.max(0)\n",
    "    else:\n",
    "        index = np.random.randint(0, len(possible_boards))\n",
    "    return int(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use: f = features(board)\n",
    "Input: board is is a 29-vector\n",
    "Output: f is a 198-vector of features that follows Tesauro's procedure.\n",
    "        See p. 423 in Sutton & Barto\n",
    "\"\"\"\n",
    "def features(board, player):\n",
    "    f = np.zeros(198)\n",
    "    \n",
    "    # define features for points on board\n",
    "    p = 0\n",
    "    for i in range(1,25):\n",
    "        point = board[i]\n",
    "        #print('point:', point)\n",
    "        #print('p: ', p)\n",
    "        if (point != 0):\n",
    "            #print('Not 0')\n",
    "            if(point > 0):\n",
    "                if (point == 1):\n",
    "                    f[p] = 1\n",
    "                elif (point == 2):\n",
    "                    f[p+1] = 1\n",
    "                elif (point == 3):\n",
    "                    f[p+2] = 1\n",
    "                else:\n",
    "                    f[p+3] = (point-3)/2\n",
    "            else:\n",
    "                if (point == -1):\n",
    "                    f[p+4] == 1\n",
    "                elif (point == -2):\n",
    "                    f[p+5] = 1\n",
    "                elif (point == -3):\n",
    "                    f[p+6] = 1\n",
    "                else:\n",
    "                    f[p+7] = (-point-3)/2\n",
    "        p += 8\n",
    "    \n",
    "    f[192] = board[25]/2\n",
    "    f[193] = board[26]/2\n",
    "    f[194] = board[27]/15\n",
    "    f[195] = board[28]/15\n",
    "    f[196] = int(player == 1)\n",
    "    f[197] = int(player == -1)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "rew_plt = []\n",
    "\n",
    "def playAgainstRandom(num_games = 100, clear = True):\n",
    "    global rew_plt\n",
    "    rew = []\n",
    "    for episode in range(num_games):\n",
    "        env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            dice = B.roll_dice()\n",
    "            for i in range(1 + int(dice[0] == dice[1])):\n",
    "                possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                if len(possible_moves) == 0:\n",
    "                    break\n",
    "                action = get_action(actor, possible_boards)\n",
    "                after_state, reward, done = env.step(possible_moves[action], player = 1)\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            if not done:\n",
    "                dice = B.roll_dice()\n",
    "                for i in range(1 + int(dice[0] == dice[1])):\n",
    "                    next_state, reward, done = env.make_move(dice)\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "\n",
    "        rew.append(reward)\n",
    "\n",
    "    if clear:\n",
    "        clear_output(True)\n",
    "    rew_plt.append(np.mean(np.equal(rew,1)))\n",
    "    rew = []\n",
    "    plt.plot(rew_plt)\n",
    "    plt.axhline(0.5, color=\"gray\")\n",
    "    plt.show()\n",
    "    print(\"Win rate:\", rew_plt[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "pubRew_plt = []\n",
    "\n",
    "def playPubeval(num_games = 100, clear = True):\n",
    "    global pubRew_plt\n",
    "    pubRew = []\n",
    "    for episode in range(num_games):\n",
    "        env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            dice = B.roll_dice()\n",
    "            for i in range(1 + int(dice[0] == dice[1])):\n",
    "                possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                if len(possible_moves) == 0:\n",
    "                    break\n",
    "                action = get_action_greedy(actor, possible_boards)\n",
    "                after_state, reward, done = env.step(possible_moves[action], player = 1)\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            if not done:\n",
    "                dice = B.roll_dice()\n",
    "                for i in range(1 + int(dice[0] == dice[1])):\n",
    "                    action = pubeval_tpr.agent_pubeval(np.copy(env.board), dice, oplayer = -1)\n",
    "                    next_state, reward, done = env.step(action, player = -1)\n",
    "                    if done:\n",
    "                        reward = -1\n",
    "                        break\n",
    "\n",
    "\n",
    "        pubRew.append(reward)\n",
    "\n",
    "    if clear:\n",
    "        clear_output(True)\n",
    "    pubRew_plt.append(np.mean(np.equal(pubRew,1)))\n",
    "    pubRew = []\n",
    "    plt.plot(pubRew_plt)\n",
    "    plt.axhline(0.5, color=\"gray\")\n",
    "    plt.show()\n",
    "    print(\"Win rate:\", pubRew_plt[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gamma = 1\n",
    "actor_alpha = 0.001\n",
    "critic_alpha = 0.001\n",
    "actor_lambda = 0.7\n",
    "critic_lambda = 0.7\n",
    "forever = 2000000\n",
    "score = []\n",
    "\n",
    "plt_iter = 200\n",
    "\n",
    "env = backgammon()\n",
    "tic = time.time()\n",
    "\n",
    "for episode in range(1,forever+1):\n",
    "\n",
    "    env.reset()\n",
    "    done = False\n",
    "    step = 1\n",
    "    \n",
    "    I = 1\n",
    "    actor_Z = [ [0 for layer in actor.parameters()] for player in range(2) ]\n",
    "    critic_Z = [ [0 for layer in critic.parameters()] for player in range(2) ]\n",
    "    value = [[0,0],[0,0]] #value[player][0(old),1(new)]\n",
    "        \n",
    "    player = 0\n",
    "    while not done:\n",
    "        dice = B.roll_dice()\n",
    "        for i in range(1 + int(dice[0] == dice[1])):\n",
    "            possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "            if len(possible_moves) == 0:\n",
    "                break\n",
    "            action, pi = get_action_and_value(actor, possible_boards) # Using actor\n",
    "            pi.clamp(min=1e-8) # so that log does not become nan\n",
    "            log_pi = torch.log(pi) \n",
    "            actor.zero_grad()\n",
    "            log_pi.backward()\n",
    "            with torch.no_grad():\n",
    "                for i, param in enumerate(actor.parameters()):\n",
    "                    actor_Z[player][i] = actor_lambda * I * actor_Z[player][i] + param.grad\n",
    "            after_state, reward, done = env.step(possible_moves[action], player = 1)\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        if not done:\n",
    "            value[player][0] = float(value[player][1]) # old_value\n",
    "            value[player][1] = get_state_value(critic, after_state)\n",
    "            critic.zero_grad()\n",
    "            value[player][1].backward()\n",
    "            with torch.no_grad():\n",
    "                for i, param in enumerate(critic.parameters()):\n",
    "                    critic_Z[player][i] = critic_lambda * critic_Z[player][i] + param.grad\n",
    "            if (step>2):\n",
    "                with torch.no_grad():\n",
    "                    reward = 0 # Reward er 0\n",
    "                    delta = reward + gamma*value[player][1] - value[player][0]\n",
    "                    \n",
    "                    for i, param in enumerate(actor.parameters()):\n",
    "                        param += actor_alpha * delta * actor_Z[player][i] \n",
    "                    for i, param in enumerate(critic.parameters()):\n",
    "                        param += critic_alpha * delta * critic_Z[player][i]\n",
    "            \n",
    "            if player==0:\n",
    "                I *= gamma\n",
    "                step += 1\n",
    "            player = 1 - player\n",
    "            env.swap_player()\n",
    "            \n",
    "    score.append(player)\n",
    "            \n",
    "    value[player][0] = float(value[player][1])\n",
    "    value[player][1] = 0\n",
    "    \n",
    "    value[1 - player][0] = float(value[1 - player][1])\n",
    "    value[1 - player][1] = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        delta1 = reward + gamma*value[player][1] - value[player][0]\n",
    "        delta2 = -reward + gamma*value[1 - player][1] - value[1 - player][0]\n",
    "        \n",
    "        for i, param in enumerate(actor.parameters()):\n",
    "            param += actor_alpha * delta1 * actor_Z[player][i]\n",
    "            param += actor_alpha * delta2 * actor_Z[1 - player][i] \n",
    "        for i, param in enumerate(critic.parameters()):\n",
    "            param += critic_alpha * delta1 * critic_Z[player][i]\n",
    "            param += critic_alpha * delta2 * critic_Z[1 - player][i]\n",
    "            \n",
    "    if episode%5000 == 0:\n",
    "        torch.save(critic.state_dict(), \"critic_nn_nott.pth\")\n",
    "        torch.save(actor.state_dict(), \"actor_nn_nott.pth\")\n",
    "\n",
    "    if episode%plt_iter == 0:\n",
    "        toc = time.time()\n",
    "        playPubeval(num_games = 30, clear = True)\n",
    "        playAgainstRandom(num_games = 30, clear = False)\n",
    "        print('++++++++++++++++++++++++++++++')\n",
    "        print('Win-rate in self-play: ', np.mean(np.equal(score,1)))\n",
    "        print(\"Time per {}: {}\".format(plt_iter, toc-tic))\n",
    "        print(\"EPISODE: \", episode, \"Steps:\", step)\n",
    "        time.sleep(50)\n",
    "        tic = time.time()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
