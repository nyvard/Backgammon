{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import Backgammon as B\n",
    "import agent as A\n",
    "import flipped_agent as FA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class backgammon:\n",
    "    def __init__(self):\n",
    "        self.board = B.init_board()\n",
    "            \n",
    "    def reset(self):\n",
    "        self.board = B.init_board()\n",
    "        self.done = False\n",
    "        \n",
    "    def choose_board(self, board):\n",
    "        self.board = board\n",
    "        self.done = False\n",
    "        return np.copy(self.board)\n",
    "    \n",
    "    def legal_moves(self, dice, player):\n",
    "        moves, boards = B.legal_moves(board = self.board, dice = dice, player = player)\n",
    "        if len(boards) == 0:\n",
    "            return [], self.board\n",
    "        return moves, np.vstack(boards)\n",
    "    \n",
    "    def swap_player(self):\n",
    "        self.board = FA.flip_board(board_copy=np.copy(self.board))\n",
    "    \n",
    "    # oppents random move\n",
    "    def make_move(self, dice):\n",
    "        moves, _ = self.legal_moves(dice, -1)\n",
    "        if len(moves) == 0:\n",
    "            return self.step([], -1)\n",
    "        move = moves[np.random.randint(len(moves))]\n",
    "        return self.step(move, -1)\n",
    "    \n",
    "    def step(self, move, player):\n",
    "        if len(move) != 0:\n",
    "            for m in move:\n",
    "                self.board = B.update_board(board = self.board, move = m, player = player)\n",
    "        reward = 0\n",
    "        self.done = False\n",
    "        if self.iswin():\n",
    "            reward = player\n",
    "            self.done = True\n",
    "        return np.copy(self.board), reward, self.done\n",
    "        \n",
    "    def iswin(self):\n",
    "        return B.game_over(self.board)\n",
    "        \n",
    "    def render(self):\n",
    "        B.pretty_print(self.board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    #tf.reset_default_graph()\n",
    "    #tf.set_random_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "D_in, H1, H2, H3, D_out = 29, 32, 64, 128, 1\n",
    "\n",
    "actor = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H1),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(H1, H2),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(H2, H3),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(H3, D_out),\n",
    "    torch.nn.Softmax(dim=0),\n",
    ")\n",
    "critic = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H1),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(H1, H2),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(H2, H3),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(H3, D_out),\n",
    "    torch.nn.Tanh(),\n",
    ")\n",
    "memory = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H1),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(H1, H2),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(H2, H3),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(H3, D_out),\n",
    "    torch.nn.Tanh(),\n",
    ")\n",
    "# save initial parameters of transient memory\n",
    "initial_memory = [copy.deepcopy(param) for param in memory.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_and_value(actor, boards):\n",
    "    boards = torch.from_numpy(boards).float()\n",
    "    possible_actions_probs = actor(boards)\n",
    "    with torch.no_grad():\n",
    "        action = int(torch.multinomial(possible_actions_probs.view(1,-1), 1))\n",
    "    action_value = possible_actions_probs[action]\n",
    "    return action, action_value\n",
    "\n",
    "def get_action_value(actor, boards, action):\n",
    "    boards = torch.from_numpy(boards).float()\n",
    "    possible_actions_probs = actor(boards)\n",
    "    action_value = possible_actions_probs[action]\n",
    "    return action_value\n",
    "\n",
    "def get_action(actor, boards):\n",
    "    with torch.no_grad():\n",
    "        boards = torch.from_numpy(boards).float()\n",
    "        possible_actions_probs = actor(boards)\n",
    "        action = torch.multinomial(possible_actions_probs.view(1,-1), 1)\n",
    "    return int(action)\n",
    "\n",
    "def get_state_value(nn_model, after_state):\n",
    "    after_state = torch.from_numpy(after_state).float()\n",
    "    value = nn_model(after_state)\n",
    "    return value\n",
    "\n",
    "def epsilon_greedy(critic, possible_boards, epsilon=.9):\n",
    "    possible_boards = torch.from_numpy(possible_boards).float()\n",
    "    values = critic(possible_boards)\n",
    "    if np.random.random()<epsilon:\n",
    "        _ , index = values.max(0)\n",
    "    else:\n",
    "        index = np.random.randint(0, len(possible_boards))\n",
    "    return int(index)\n",
    "\n",
    "def composite_greedy(critic, memory, possible_boards, epsilon=1):\n",
    "    possible_boards = torch.from_numpy(possible_boards).float()\n",
    "    critic_values = critic(possible_boards)\n",
    "    memory_values = memory(possible_boards)\n",
    "    values = critic_values + memory_values\n",
    "    if np.random.random()<epsilon:\n",
    "        _ , index = values.max(0)\n",
    "    else:\n",
    "        index = np.random.randint(0, len(possible_boards))\n",
    "    return int(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use: valuefn_temp = search(pre_state, memory, n_dreams, max_steps)\n",
    "Input: pre_state is current state,\n",
    "       memory is the transient memorythe value function,\n",
    "       old_value is the value of last after state\n",
    "       n_dreams is number of dreams,\n",
    "       max_steps is maximum number of steps,\n",
    "Output: memory has been updated for episodes in dream\n",
    "\"\"\"\n",
    "def search(pre_state, pre_value, n_dreams, max_steps = 1000):\n",
    "    # Clear eligibility trace\n",
    "    with torch.no_grad():\n",
    "        for i, param in enumerate(memory.parameters()):\n",
    "            param.data.copy_(initial_memory[i])\n",
    "    \n",
    "    \n",
    "    # Dream n_dreams\n",
    "    for dreams in range(n_dreams):\n",
    "        # Clear eligibility trace\n",
    "        memory_Z = [0 for layer in critic.parameters()]\n",
    "        done = False\n",
    "        I = 1\n",
    "        step = 1\n",
    "        state = env.choose_board(np.copy(pre_state))\n",
    "        #Set after_state = pre_state to deal with possibilty of no legal move on first step\n",
    "        #after_state = env.choose_board(np.copy(pre_state))\n",
    "        old_value = pre_value\n",
    "        skip_round = False\n",
    "        Error = False\n",
    "        \n",
    "        # play one round and update\n",
    "        while not (done or step > max_steps):\n",
    "            dice = B.roll_dice()\n",
    "            for i in range(1 + int(dice[0] == dice[1])):\n",
    "                possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                if len(possible_moves) == 0:\n",
    "                    after_state = np.copy(state)\n",
    "                    #skip_round = True\n",
    "                    #after_state = next_state\n",
    "                    break\n",
    "                # Use composite value function to choose action\n",
    "                action = composite_greedy(critic, memory, possible_boards)\n",
    "                after_state, reward, done = env.step(possible_moves[action], player = 1)\n",
    "                if done:\n",
    "                    break\n",
    "            if not done:\n",
    "                critic_value = get_state_value(critic, after_state)\n",
    "                memory_value = get_state_value(memory, after_state)\n",
    "                value = critic_value + memory_value\n",
    "                #calc critic gradient\n",
    "                memory.zero_grad()\n",
    "                critic.zero_grad()\n",
    "                value.backward()\n",
    "                with torch.no_grad():\n",
    "                    for i, param in enumerate(memory.parameters()):\n",
    "                        memory_Z[i] = memory_lambda * memory_Z[i] + param.grad\n",
    "            else:\n",
    "                value = 0\n",
    "            skip_round = False\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # other players move\n",
    "                if not done:\n",
    "                    dice = B.roll_dice()\n",
    "                    for i in range(1 + int(dice[0] == dice[1])):\n",
    "                        next_state, reward, done = env.make_move(dice)\n",
    "                        if done:\n",
    "                            break\n",
    "                    #next_value = get_state_value(critic, next_state)\n",
    "                else:\n",
    "                    next_value = 0\n",
    "                \n",
    "                delta = reward + gamma*value - old_value\n",
    "                old_value = value\n",
    "            \n",
    "                # apply gradients\n",
    "                for i, param in enumerate(memory.parameters()):\n",
    "                    param += memory_alpha * delta * memory_Z[i]\n",
    "\n",
    "            I *= gamma\n",
    "            step +=1\n",
    "\n",
    "    state = env.choose_board(pre_state)\n",
    "    old_value = pre_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward:  1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 200\n",
      "time per 100 : 345.32199907302856\n",
      "EPISODE:  220\n",
      "EPISODE:  240\n",
      "EPISODE:  260\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.99\n",
    "actor_alpha = 0.01\n",
    "critic_alpha = 0.01\n",
    "memory_alpha = 0.01\n",
    "actor_lambda = 0.7\n",
    "critic_lambda = 0.7\n",
    "memory_lambda = 0.7\n",
    "forever = 1000\n",
    "\n",
    "plt_iter = 100\n",
    "rew = []\n",
    "rew_plt = []\n",
    "\n",
    "from time import time\n",
    "tic = time()\n",
    "\n",
    "for episode in range(forever):\n",
    "    env = backgammon()\n",
    "    done = False\n",
    "    I = 1\n",
    "    step = 1\n",
    "    actor_Z = [0 for layer in actor.parameters()]\n",
    "    critic_Z = [0 for layer in critic.parameters()]\n",
    "    memory_Z = [0 for layer in memory.parameters()]\n",
    "    n_dreams = 5\n",
    "    max_rounds = 5\n",
    "    Error = False\n",
    "    \n",
    "    if (episode%20==0):\n",
    "        print(\"EPISODE: \", episode)\n",
    "\n",
    "    while not done:\n",
    "        dice = B.roll_dice()\n",
    "        for i in range(1 + int(dice[0] == dice[1])):\n",
    "            possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "            if len(possible_moves) == 0:\n",
    "                break\n",
    "            if (step > 1):\n",
    "                pre_state = np.copy(env.board)\n",
    "                search(pre_state, old_value, n_dreams, max_rounds)\n",
    "                action = composite_greedy(critic, memory, possible_boards)\n",
    "            else:\n",
    "                action = epsilon_greedy(critic, possible_boards) # No search on first step                    \n",
    "            after_state, reward, done = env.step(possible_moves[action], player = 1)\n",
    "            if done:\n",
    "                break\n",
    "        if not done:\n",
    "            value = get_state_value(critic, after_state)\n",
    "            critic.zero_grad()\n",
    "            value.backward()\n",
    "            with torch.no_grad():\n",
    "                for i, param in enumerate(critic.parameters()):\n",
    "                    critic_Z[i] = critic_lambda * critic_Z[i] + param.grad\n",
    "        else:\n",
    "            value = 0\n",
    "                  \n",
    "        with torch.no_grad():\n",
    "            # other players move\n",
    "            if not done:\n",
    "                dice = B.roll_dice()\n",
    "                for i in range(1 + int(dice[0] == dice[1])):\n",
    "                    next_state, reward, done = env.make_move(dice)\n",
    "                    if done:\n",
    "                        break\n",
    "            else:\n",
    "                next_value = 0\n",
    "            if (step>1):\n",
    "                delta = reward + gamma*value - old_value\n",
    "            old_value = value\n",
    "\n",
    "            ###### plot\n",
    "            if episode%plt_iter == 0:\n",
    "                if done:\n",
    "                    clear_output(True)\n",
    "                    print('Reward: ',reward)\n",
    "                    rew_plt.append(np.mean(np.equal(rew,1)))\n",
    "                    rew = []\n",
    "                    plt.plot(rew_plt)\n",
    "                    plt.axhline(0.5, color=\"gray\")\n",
    "                    plt.show()\n",
    "                    rnd = False\n",
    "                    print(\"Episode: {}\".format(episode))\n",
    "                    toc=time()\n",
    "                    print('time per',plt_iter,':',toc-tic)\n",
    "                    tic=toc\n",
    "                    #env.render()\n",
    "            ######\n",
    "            \n",
    "            if step > 1:\n",
    "                for j, param in enumerate(critic.parameters()):\n",
    "                    param += critic_alpha * delta * critic_Z[j]        \n",
    "        I *= gamma\n",
    "        step += 1\n",
    "        \n",
    "    rew.append(reward)\n",
    "    #actor_alpha *= 0.99\n",
    "    #critic_alpha *= 0.99"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
