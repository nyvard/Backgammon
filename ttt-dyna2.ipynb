{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "class tictactoe():\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros(9, dtype='int')\n",
    "        \n",
    "    def reset(self):\n",
    "        self.board[:] = 0\n",
    "        self.done = False\n",
    "        return np.copy(self.board)\n",
    "    \n",
    "    def choose_board(self, board):\n",
    "        self.board = board\n",
    "        self.done = False\n",
    "        return np.copy(self.board)\n",
    "    \n",
    "    def legal_moves(self, player):\n",
    "        moves = np.where(self.board == 0)[0]\n",
    "        boards = []\n",
    "        for move in moves:\n",
    "            board = np.copy(self.board)\n",
    "            board[move] = player\n",
    "            boards.append(board)\n",
    "        return moves, np.array(boards, dtype='double')\n",
    "    \n",
    "    def swap_player(self):\n",
    "        self.board = -self.board\n",
    "    \n",
    "    # oppents random move\n",
    "    def make_move(self, player=-1):\n",
    "        moves, _ = self.legal_moves(player)\n",
    "        return self.step(np.random.choice(moves, 1),player)\n",
    "    \n",
    "    def step(self, move, player=1):\n",
    "        assert self.board[move] == 0, \"Tried to play an illegal move, player = %d\"%player\n",
    "        assert not self.done, \"Game has finished must call tictactoe.reset()\"\n",
    "        self.board[move] = player\n",
    "        reward = 0\n",
    "        self.done = False\n",
    "        if self.iswin(player):\n",
    "            reward = 1\n",
    "            self.done = True\n",
    "        if not np.any(self.board==0):\n",
    "            self.done = True\n",
    "        return np.copy(self.board), reward, self.done\n",
    "        \n",
    "    def iswin(self, player):\n",
    "        for i in range(3):\n",
    "            if np.all(self.board[[i*3, i*3+1, i*3+2]]==player) | np.all(self.board[[i, i+3, i+6]]==player):\n",
    "                return True\n",
    "        if np.all(self.board[[0, 4, 8]] == player) | np.all(self.board[[2, 4, 6]] == player):\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    def render(self):\n",
    "        data_mat = self.board.reshape(3, 3)\n",
    "        for i in range(0, 3):\n",
    "            print('-------------')\n",
    "            out = '| '\n",
    "            for j in range(0, 3):\n",
    "                token = \"\"\n",
    "                if data_mat[i, j] == 1:\n",
    "                    token = 'x'\n",
    "                if data_mat[i, j] == 0:\n",
    "                    token = ' '\n",
    "                if data_mat[i, j] == -1:\n",
    "                    token = 'o'\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-------------')\n",
    "        \n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    #tf.reset_default_graph()\n",
    "    #tf.set_random_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "reset_graph()\n",
    "\n",
    "\n",
    "# D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "D_in, H, D_out = 9, 50, 1\n",
    "\n",
    "actor = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    "    torch.nn.Softmax(dim=0),\n",
    ")\n",
    "critic = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    "    torch.nn.Tanh(),\n",
    ")\n",
    "memory = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    "    torch.nn.Tanh(),\n",
    ")\n",
    "\n",
    "# save initial parameters of transient memory\n",
    "initial_memory = [copy.deepcopy(param) for param in memory.parameters()]\n",
    "\n",
    "def get_action_and_value(actor, boards):\n",
    "    boards = torch.from_numpy(boards).float()\n",
    "    possible_actions_probs = actor(boards)\n",
    "    action = int(torch.multinomial(possible_actions_probs.view(1,-1), 1))\n",
    "    action_value = possible_actions_probs[action]\n",
    "    return action, action_value\n",
    "    \n",
    "def get_action_value(actor, boards, action):\n",
    "    boards = torch.from_numpy(boards).float()\n",
    "    possible_actions_probs = actor(boards)\n",
    "    action_value = possible_actions_probs[action]\n",
    "    return action_value\n",
    "\n",
    "def get_action(actor, boards):\n",
    "    boards = torch.from_numpy(boards).float()\n",
    "    possible_actions_probs = actor(boards)\n",
    "    action = torch.multinomial(possible_actions_probs.view(1,-1), 1)\n",
    "    return int(action)\n",
    "\n",
    "def get_state_value(nn_model, after_state):\n",
    "    after_state = torch.from_numpy(after_state).float()\n",
    "    value = nn_model(after_state)\n",
    "    return value\n",
    "\n",
    "def get_composite_value(critic, memory, after_state):\n",
    "    after_state = torch.from_numpy(after_state).float()\n",
    "    critic_value = critic(after_state)\n",
    "    memory_value = memory(after_state)\n",
    "    value = critic_value + memory_value\n",
    "    return value\n",
    "    \n",
    "\n",
    "def epsilon_greedy(critic, possible_boards, epsilon=1):\n",
    "    possible_boards = torch.from_numpy(possible_boards).float()\n",
    "    values = critic(possible_boards)\n",
    "    if np.random.random()<epsilon:\n",
    "        _ , index = values.max(0)\n",
    "    else:\n",
    "        index = np.random.randint(0, len(possible_boards))\n",
    "    return int(index)\n",
    "\n",
    "def composite_greedy(critic, memory, possible_boards, epsilon=1):\n",
    "    possible_boards = torch.from_numpy(possible_boards).float()\n",
    "    critic_values = critic(possible_boards)\n",
    "    memory_values = memory(possible_boards)\n",
    "    values = critic_values + memory_values\n",
    "    if np.random.random()<epsilon:\n",
    "        _ , index = values.max(0)\n",
    "    else:\n",
    "        index = np.random.randint(0, len(possible_boards))\n",
    "    return int(index)\n",
    "        \n",
    "        \n",
    "gamma = .1\n",
    "critic_alpha = 0.05\n",
    "memory_alpha = 0.05\n",
    "\n",
    "critic_lambda = 0.9\n",
    "memory_lambda = 0.9\n",
    "\n",
    "critic_Z = [0 for layer in critic.parameters()]\n",
    "memory_Z = [0 for layer in memory.parameters()]\n",
    "\n",
    "plt_iter = 1000\n",
    "rew = []\n",
    "rew_plt = []\n",
    "\n",
    "from time import time\n",
    "tic = time()\n",
    "\n",
    "env = tictactoe()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Use: valuefn_temp = search(pre_state, memory, n_dreams, max_steps)\n",
    "Input: pre_state is current state,\n",
    "       memory is the transient memorythe value function,\n",
    "       old_value is the value of last after state\n",
    "       n_dreams is number of dreams,\n",
    "       max_steps is maximum number of steps,\n",
    "Output: memory has been updated for episodes in dream\n",
    "\"\"\"\n",
    "def search(pre_state, pre_value, n_dreams, max_steps = 1000):\n",
    "    # Clear transient memory and eligibility trace\n",
    "    memory_Z = [0 for layer in critic.parameters()]\n",
    "    with torch.no_grad():\n",
    "        for i, param in enumerate(memory.parameters()):\n",
    "            param.data.copy_(initial_memory[i])\n",
    "    \n",
    "    # Dream n_dreams\n",
    "    for dreams in range(n_dreams):\n",
    "        done = False\n",
    "        memory_Z = [0 for layer in critic.parameters()]\n",
    "        I = 1\n",
    "        step = 1\n",
    "        state = env.choose_board(np.copy(pre_state))\n",
    "        old_value = pre_value\n",
    "        \n",
    "        #print('dream: ', dreams)\n",
    "        #print('pre_state: ', pre_state)\n",
    "        #env.render()\n",
    "        \n",
    "        # play one round and update\n",
    "        while not (done or step > max_steps):\n",
    "            #print('step: ', step)\n",
    "            possible_moves, possible_boards = env.legal_moves(1)\n",
    "            #env.render()\n",
    "            # Use composite value function to choose action\n",
    "            action = composite_greedy(critic, memory, possible_boards)\n",
    "        \n",
    "            after_state, reward, done = env.step(possible_moves[action])\n",
    "        \n",
    "            if not done:\n",
    "                critic_value = get_state_value(critic, after_state)\n",
    "                memory_value = get_state_value(memory, after_state)\n",
    "                value = critic_value + memory_value\n",
    "                #calc critic gradient\n",
    "                memory.zero_grad()\n",
    "                critic.zero_grad()\n",
    "                value.backward()\n",
    "                with torch.no_grad():\n",
    "                    for i, param in enumerate(memory.parameters()):\n",
    "                        memory_Z[i] = memory_lambda * memory_Z[i] + param.grad\n",
    "            else:\n",
    "                value = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # other players move\n",
    "                if not done:\n",
    "                    next_state, reward, done = env.make_move()\n",
    "                    reward = -reward\n",
    "                    next_value = get_state_value(critic, next_state)\n",
    "                else:\n",
    "                    next_value = 0\n",
    "                \n",
    "                delta = reward + gamma*value - old_value\n",
    "                old_value = value\n",
    "            \n",
    "                # apply gradients\n",
    "                for i, param in enumerate(memory.parameters()):\n",
    "                    param += memory_alpha * delta * memory_Z[i]\n",
    "\n",
    "            I *= gamma\n",
    "            step +=1\n",
    "\n",
    "    state = env.choose_board(pre_state)\n",
    "    old_value = pre_value\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after state\n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "| x |   |   | \n",
      "-------------\n",
      "currnt state\n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "| x | o |   | \n",
      "-------------\n",
      "critic_value:  tensor([0.3140], grad_fn=<TanhBackward>)\n",
      "memory_value:  tensor([-0.0975], grad_fn=<TanhBackward>)\n",
      "action:  2\n",
      "critic_action:  8\n",
      "after state\n",
      "-------------\n",
      "|   |   | x | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "| x | o |   | \n",
      "-------------\n",
      "currnt state\n",
      "-------------\n",
      "|   | o | x | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "| x | o |   | \n",
      "-------------\n",
      "critic_value:  tensor([0.1909], grad_fn=<TanhBackward>)\n",
      "memory_value:  tensor([-0.3758], grad_fn=<TanhBackward>)\n",
      "action:  4\n",
      "critic_action:  4\n",
      "after state\n",
      "-------------\n",
      "|   | o | x | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "| x | o |   | \n",
      "-------------\n",
      "after state\n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "| x |   |   | \n",
      "-------------\n",
      "currnt state\n",
      "-------------\n",
      "|   |   | o | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "| x |   |   | \n",
      "-------------\n",
      "critic_value:  tensor([0.3549], grad_fn=<TanhBackward>)\n",
      "memory_value:  tensor([-0.2631], grad_fn=<TanhBackward>)\n",
      "action:  1\n",
      "critic_action:  8\n",
      "after state\n",
      "-------------\n",
      "|   | x | o | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "| x |   |   | \n",
      "-------------\n",
      "currnt state\n",
      "-------------\n",
      "|   | x | o | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "| x |   | o | \n",
      "-------------\n",
      "critic_value:  tensor([0.3103], grad_fn=<TanhBackward>)\n",
      "memory_value:  tensor([-0.2194], grad_fn=<TanhBackward>)\n",
      "action:  5\n",
      "critic_action:  4\n",
      "after state\n",
      "-------------\n",
      "|   | x | o | \n",
      "-------------\n",
      "|   |   | x | \n",
      "-------------\n",
      "| x |   | o | \n",
      "-------------\n",
      "currnt state\n",
      "-------------\n",
      "|   | x | o | \n",
      "-------------\n",
      "|   | o | x | \n",
      "-------------\n",
      "| x |   | o | \n",
      "-------------\n",
      "critic_value:  tensor([0.1219], grad_fn=<TanhBackward>)\n",
      "memory_value:  tensor([-0.8815], grad_fn=<TanhBackward>)\n",
      "action:  3\n",
      "critic_action:  3\n",
      "after state\n",
      "-------------\n",
      "|   | x | o | \n",
      "-------------\n",
      "| x | o | x | \n",
      "-------------\n",
      "| x |   | o | \n",
      "-------------\n",
      "currnt state\n",
      "-------------\n",
      "|   | x | o | \n",
      "-------------\n",
      "| x | o | x | \n",
      "-------------\n",
      "| x | o | o | \n",
      "-------------\n",
      "critic_value:  tensor([-0.2845], grad_fn=<TanhBackward>)\n",
      "memory_value:  tensor([-0.0619], grad_fn=<TanhBackward>)\n",
      "action:  0\n",
      "critic_action:  0\n",
      "after state\n",
      "-------------\n",
      "| x | x | o | \n",
      "-------------\n",
      "| x | o | x | \n",
      "-------------\n",
      "| x | o | o | \n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "forever = 2\n",
    "for episode in range(forever):      \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    critic_Z = [0 for layer in critic.parameters()]\n",
    "    I = 1\n",
    "    step = 1\n",
    "    n_dreams = 50\n",
    "    max_steps = 10\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        possible_moves, possible_boards = env.legal_moves(1)\n",
    "        if (step > 1):\n",
    "            search(next_state, old_value, n_dreams, max_steps)\n",
    "            #env.render()\n",
    "            action = composite_greedy(critic, memory, possible_boards)\n",
    "            \n",
    "            print('currnt state')\n",
    "            env.render()\n",
    "            critic_value = get_state_value(critic, after_state)\n",
    "            memory_value = get_state_value(memory, after_state)\n",
    "            print('critic_value: ', critic_value)\n",
    "            print('memory_value: ', memory_value)\n",
    "            critic_action = epsilon_greedy(critic, possible_boards)\n",
    "            print('action: ', possible_moves[action])\n",
    "            print('critic_action: ', possible_moves[critic_action])\n",
    "            \n",
    "        else:\n",
    "            action = epsilon_greedy(critic, possible_boards) # No search on first step\n",
    "        \n",
    "        after_state, reward, done = env.step(possible_moves[action])\n",
    "        print('after state')\n",
    "        env.render()\n",
    "        \n",
    "        if not done:\n",
    "            value = get_state_value(critic, after_state)\n",
    "            #calc critic gradient\n",
    "            critic.zero_grad()\n",
    "            value.backward()\n",
    "            with torch.no_grad():\n",
    "                for i, param in enumerate(critic.parameters()):\n",
    "                    critic_Z[i] = critic_lambda * critic_Z[i] + param.grad\n",
    "        else:\n",
    "            value = 0\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            # other players move\n",
    "            if not done:\n",
    "                next_state, reward, done = env.make_move()\n",
    "                reward = -reward\n",
    "                next_value = get_state_value(critic, next_state)\n",
    "            else:\n",
    "                next_value = 0\n",
    "                \n",
    "            if step > 1:\n",
    "                delta = reward + gamma*value - old_value\n",
    "            \n",
    "            old_value = value\n",
    "        \n",
    "            ###### plot\n",
    "            \"\"\"\n",
    "            if episode%plt_iter == 0:\n",
    "                env.render()\n",
    "                if done:\n",
    "                    print('Reward: ',reward)\n",
    "                    rew_plt.append(np.mean(np.equal(rew,-1)))\n",
    "                    rew = []\n",
    "                    plt.plot(rew_plt)\n",
    "                    plt.show()\n",
    "                    rnd = False\n",
    "                    print(\"Episode: {}\".format(episode))\n",
    "                    toc=time()\n",
    "                    print('time per',plt_iter,':',toc-tic)\n",
    "                    tic=toc\n",
    "            \"\"\"\n",
    "            ######\n",
    "            \n",
    "            # apply gradients\n",
    "            if step > 1:\n",
    "                for i, param in enumerate(critic.parameters()):\n",
    "                    param += critic_alpha * delta * critic_Z[i]\n",
    " \n",
    "            \n",
    "        I *= gamma\n",
    "        step +=1\n",
    "        \n",
    "    rew.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = tictactoe()\n",
    "possible_moves, possible_boards = env.legal_moves(1)\n",
    "b = np.array([1,  0, -1,  0,  1, -1, 0, -1,  1])\n",
    "c = env.choose_board(b)\n",
    "possible_moves, possible_boards = env.legal_moves(1)\n",
    "possible_boards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.arrya([1,  0, -1,  1,  1, -1, 0, -1,  1]\n",
    "a = u.choose_board(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
